---
title: "Untitled"
output:
  word_document: default
  html_document: default
---


Siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:
1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

El hundimiento del Titanic es uno de los naufragios más infames de la historia.

El 15 de abril de 1912, durante su viaje inaugural, el ampliamente considerado "insumergible" RMS Titanic se hundió después de chocar con un iceberg. Desafortunadamente, no había suficientes botes salvavidas para todos a bordo, resultando en la muerte de 1502 de 2224 pasajeros y tripulación.

Aunque había un elemento de suerte en la supervivencia, parece que algunos grupos de personas tenían más probabilidades de sobrevivir que otros.

En este desafío, les pedimos que construyan un modelo predictivo que responda a la pregunta: "¿qué tipo de personas tenían más probabilidades de sobrevivir?" usando los datos de los pasajeros (es decir, nombre, edad, sexo, clase socioeconómica, etc.).

usar machine learning para crear un modelo que prediga qué pasajeros sobrevivieron al naufragio del Titanic.


2. Integración y selección de los datos de interés a analizar.

La integración (combinación de datos de distintas fuentes), selección (filtrado de los datos de interés) y reducción (representación reducida de los datos manteniendo la integridad de la muestra original) corresponden a la limpieza de los datos. 

Los datos de los que disponemos son:
Train.csv: contiene los detalles () de un subconjunto de los pasajeros a bordo (891 para ser exactos) y, además revela si sobrevivieron o no.
Test.csv: contiene información similar al train.csv pero no revela si cada pasajero sobrevivió o no al desastre. 

Si no queremos hacer predicciones sobre la supervivencia de los pasajeros del test.csv podemos integrar los dos datasets para hacer otro tipo de tests estadisticos. 

```{r}
train <- read.csv('C:/Users/ester/Desktop/Kaggle/train.csv')
test <- read.csv('C:/Users/ester/Desktop/Kaggle/test.csv')
datos <- rbind(train[,-2], test)
```

3. Limpieza de los datos.


Ademas creamos alguna nueva variable que puede ser interesante para los estudios estadisticos posteriores.
```{r}
head(train)
str(train)
train$Pclass <- factor(train$Pclass, levels = c(1,2,3), labels = c('First', 'Second', 'Third'))
train$Sex <- factor(train$Sex, levels = c('male','female'), labels = c('Male', 'Female'))
train$Embarked <- factor(train$Embarked, levels = c('C','Q','S'), labels = c('Cherbourg', 'Queenstown', 'Southampton'))
str(train)
summary(train)
```

```{r}
strsplit(train$Name, split = ' ')[2]
formula <- unlist(sapply(strsplit(train$Name, ", "), function(x) x[2], simplify=FALSE))
train$formula1 <- unlist(sapply(strsplit(formula, ". "), function(x) x[1], simplify=FALSE))
train$Family_Name <- unlist(sapply(strsplit(train$Name, ". "), function(x) x[1], simplify=FALSE))
```


```{r}
head(test)
str(test)
test$Pclass <- factor(test$Pclass, levels = c(1,2,3), labels = c('First', 'Second', 'Third'))
test$Sex <- factor(test$Sex, levels = c('male','female'), labels = c('Male', 'Female'))
test$Embarked <- factor(test$Embarked, levels = c('C','Q','S'), labels = c('Cherbourg', 'Queenstown', 'Southampton'))
str(test)
summary(test)
```


```{r}
strsplit(test$Name, split = ' ')[2]
formula <- unlist(sapply(strsplit(test$Name, ", "), function(x) x[2], simplify=FALSE))
test$formula1 <- unlist(sapply(strsplit(formula, ". "), function(x) x[1], simplify=FALSE))
test$Family_Name <- unlist(sapply(strsplit(test$Name, ". "), function(x) x[1], simplify=FALSE))
```

```{r}
head(datos)
str(datos)
datos$Pclass <- factor(datos$Pclass, levels = c(1,2,3), labels = c('First', 'Second', 'Third'))
datos$Sex <- factor(datos$Sex, levels = c('male','female'), labels = c('Male', 'Female'))
datos$Embarked <- factor(datos$Embarked, levels = c('C','Q','S'), labels = c('Cherbourg', 'Queenstown', 'Southampton'))
str(datos)
summary(datos)
```

```{r}
strsplit(datos$Name, split = ' ')[2]
formula <- unlist(sapply(strsplit(datos$Name, ", "), function(x) x[2], simplify=FALSE))
datos$formula1 <- unlist(sapply(strsplit(formula, ". "), function(x) x[1], simplify=FALSE))
datos$Family_Name <- unlist(sapply(strsplit(datos$Name, ". "), function(x) x[1], simplify=FALSE))
```



3.1. ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

Vemos que algunas variables contienen ceros y/o elementos vacios. 

Las variables `SibSp`, `Parch` y `Fare` contienen ceros.

La variable `SibSp` hace referencia al numero de hermanos/conyuges a bordo del Titanic, con que los ceros tienen sentido y entran dentro del rango de la variable.

La variable `Pacrh` hace referencia al numero de padre e hijos a bordo del Titanic, con lo cual los ceros tmb tienen sentido y entran dentro del rango de varlores admisibles para la variable. 

La variable `Fare` hace referencia a la tarifa que pagaron los pasajeros por su ticket. Aparecen 17 valores 0, no sabamos si es un error o esos pasajeros viajaron gratis. 

Entre la gente que que viajaba en el Titanic habia tripulacion y pasajeros, podemos suponer que esos 17 0 que aparecen son debidos a la tripulacion que aparece en el dataset.


```{r}
boxplot(datos$Fare)
hist(datos$Fare)
plot(table(datos$Fare))
```

```{r}
datos_0 <- datos[datos$Fare == 0,]
head(datos_0)
```


Todos son varones, mayores de edad y embarcaron en el puerto de Southampton.
Cuando buscamos informacion de estos pasajeros veemos que algunos pertenecian al Titanic Guarantee Group (El equipo de Belfast enviado por los constructores de barcos Harland & Wolff para acompañar al Titanic en su viaje inaugural), con lo cual podemos suponer que los 0 son correctos, era gente que estaba viajando gratis.


Vemos que las variables `Age`, `Fare` y `Embarked` contienen valores perdidos. 
https://conocemachinelearning.wordpress.com/2017/06/30/valoresfaltantes/
http://eio.usc.es/pub/mte/descargas/ProyectosFinMaster/Proyecto_616.pdf


Las ventajas de imputar son que logramos obtener un conjunto de datos completo sin datos faltantes, se puede reducir el sesgo debido a la no respuesta y la imputaci´on opera sobre los datos, de forma que los resultados obtenidos por los diferentes an´alisis son mutuamente consistentes.
Por otra parte, la imputaci´on tambi´en tiene desventajas ya que hay que tener en cuenta que el futuro an´alisis no distingue entre las imputaciones y los datos reales. Adem´as los valores imputados pueden ser buenas estimaciones pero no son datos reales y no podemos asegurar una mejora en el sesgo respecto del sistema de datos incompletos. Al fin y al cabo la imputaci´on es un procedimiento de generar datos.
Si el m´etodo de imputaci´on no es el adecuado, posiblemente aumente el sesgo y sobreestime la varianza, obteniendo datos imputados inconsistentes produciendo una base de datos no confiables, llevando a la interpretaci´on err´onea de los resultados por parte de los usuarios.

Las variables `Fare` y `Embarked` tienen 1 y 2 valores perdidos respectivamente, como la muestras es bastante grande no hace falta imputador datos.
Pero en la variable `Age` faltan 263 valores del 1309, representa un 20% de los datos, ademas un 20% es el maximo de valores perdidos para los que algunos autores recomiendan la imputacion de datos. 

*Realizaremos una imputacion por Knn


```{r}

```

3.2. Identificación y tratamiento de valores extremos.

Los valores extremos o outliers son aquellos datos que se encuentran muy alejados de la distribución normal de una variable o población. Al ser observaciones que se desvían del resto levantan sospechas sobre si fueron generadas mediante el mismo mecanismo.

La decisión sobre qué se considera un valor extremo puede resultar controvertida, pero generalmente se considera que un valor es extremo cuando el valor se encuentra alejado 3 desviaciones estándar con respecto a la media del conjunto de datos.
Por ello, normalmente se utiliza la representación de los datos mediante gráficos de cajas (boxplots) con el objetivo de detectar dichos outliers. Otros métodos que permiten detectar los valores extremos se basan en la distancia de Mahalanobis o la distancia de Cook.

Sus posibles efectos son:
- incrementar el error en la varianza de los datos
- sesgar los cálculos y estimaciones.

```{r}
boxplot(datos$Age)
boxplot(datos$Age~datos$Sex)
boxplot(datos$Age~datos$Pclass)
boxplot(datos$SibSp)
boxplot(datos$SibSp~datos$Pclass)
boxplot(datos$Parch)
boxplot(datos$Parch~datos$Pclass)
boxplot(datos$Fare~datos$Pclass)
```

Los valores extremos (outliers) pueden aparecer por distintos motivos.
- son valores validos que forman parte de la muestra
- son valores debidos a una desviacion sistematica en el grupo de valores extremos
- son errors en los datos

En este caso parece que los ouliers son valores son válidos y entran dentro del rango de las variables, por tanto, forman parte de la muestra, por lo que no se deben modificar ni eliminar, y se deben tener en cuenta en el análisis de los datos.

4. Análisis de los datos.

4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

- Predecir que pasajeros del dataset test sobrevivieron, usando el dataset train para entrenar un algoritmo de clasificacion. En este caso parece interesante utilizar decicion trees, ya que nos da la informacion de como se clasificacn los pasajeros y nos devuelve un grafico el mismo.

- Mediante un mpodelo de regresion lineal para predecir que probabilidad hay de que un pasajero sobreviva en base a sus caracteristicas.

- Diferencias entre la edad de los pasajeros de cada sexo (ttest). 


4.2. Comprobación de la normalidad y homogeneidad de la varianza.

* Comprobacion de la normalidad:
```{r}
hist(datos$Age)
boxplot(datos$Age)

library(ggplot2)
datos1 <- na.omit(datos)
ggplot(data = datos1, aes(x = Age)) +
geom_histogram(aes(y = ..density.., fill = ..count..)) +
scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
stat_function(fun = dnorm, colour = "firebrick",args = list(mean = mean(datos1$Age),sd = sd(datos1$Age))) +
ggtitle("Histograma con curva normal teórica") +
theme_bw()

qqnorm(datos$Age, pch = 19, col = "gray50")
qqline(datos$Age)
```

```{r}
hist(datos$Fare)
boxplot(datos$Fare)

library(ggplot2)
datos1 <- na.omit(datos)
ggplot(data = datos1, aes(x = Fare)) +
geom_histogram(aes(y = ..density.., fill = ..count..)) +
scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
stat_function(fun = dnorm, colour = "firebrick",args = list(mean = mean(datos1$Fare),sd = sd(datos1$Fare))) +
ggtitle("Histograma con curva normal teórica") +
theme_bw()

qqnorm(datos$Fare, pch = 19, col = "gray50")
qqline(datos$Fare)

```

El test Lilliefors asume que la media y varianza son desconocidas, estando especialmente desarrollado para contrastar la normalidad. Es la alternativa al test de Shapiro-Wilk cuando el número de observaciones es mayor de 50, como es nuestro caso.

```{r}
library("nortest")
lillie.test(x = datos$Age)
lillie.test(x = datos$Fare)
```

* Comprobacion homogeneidad de la varianza:


4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

5. Representación de los resultados a partir de tablas y gráficas.

6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?




